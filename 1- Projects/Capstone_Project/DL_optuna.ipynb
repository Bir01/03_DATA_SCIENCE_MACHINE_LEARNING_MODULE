{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhCAv0lsnRA-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"HR_Dataset.csv\")"
      ],
      "metadata": {
        "id": "2qz3GFdFnb9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CD2byIEXnd8q",
        "outputId": "0566b191-9b39-46cf-b890-1293cd99d372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
              "0                0.38             0.53               2                   157   \n",
              "1                0.80             0.86               5                   262   \n",
              "2                0.11             0.88               7                   272   \n",
              "3                0.72             0.87               5                   223   \n",
              "4                0.37             0.52               2                   159   \n",
              "\n",
              "   time_spend_company  Work_accident  left  promotion_last_5years  \\\n",
              "0                   3              0     1                      0   \n",
              "1                   6              0     1                      0   \n",
              "2                   4              0     1                      0   \n",
              "3                   5              0     1                      0   \n",
              "4                   3              0     1                      0   \n",
              "\n",
              "  Departments   salary  \n",
              "0        sales     low  \n",
              "1        sales  medium  \n",
              "2        sales  medium  \n",
              "3        sales     low  \n",
              "4        sales     low  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca645615-8538-4be4-9a83-077b92dc968a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>satisfaction_level</th>\n",
              "      <th>last_evaluation</th>\n",
              "      <th>number_project</th>\n",
              "      <th>average_montly_hours</th>\n",
              "      <th>time_spend_company</th>\n",
              "      <th>Work_accident</th>\n",
              "      <th>left</th>\n",
              "      <th>promotion_last_5years</th>\n",
              "      <th>Departments</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.53</td>\n",
              "      <td>2</td>\n",
              "      <td>157</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>sales</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>5</td>\n",
              "      <td>262</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>sales</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.88</td>\n",
              "      <td>7</td>\n",
              "      <td>272</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>sales</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.72</td>\n",
              "      <td>0.87</td>\n",
              "      <td>5</td>\n",
              "      <td>223</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>sales</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.52</td>\n",
              "      <td>2</td>\n",
              "      <td>159</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>sales</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca645615-8538-4be4-9a83-077b92dc968a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca645615-8538-4be4-9a83-077b92dc968a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca645615-8538-4be4-9a83-077b92dc968a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh_B0cyKnf0B",
        "outputId": "c5827fe0-f04b-42e2-91cd-81b1a8d7eb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14999 entries, 0 to 14998\n",
            "Data columns (total 10 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   satisfaction_level     14999 non-null  float64\n",
            " 1   last_evaluation        14999 non-null  float64\n",
            " 2   number_project         14999 non-null  int64  \n",
            " 3   average_montly_hours   14999 non-null  int64  \n",
            " 4   time_spend_company     14999 non-null  int64  \n",
            " 5   Work_accident          14999 non-null  int64  \n",
            " 6   left                   14999 non-null  int64  \n",
            " 7   promotion_last_5years  14999 non-null  int64  \n",
            " 8   Departments            14999 non-null  object \n",
            " 9   salary                 14999 non-null  object \n",
            "dtypes: float64(2), int64(6), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.salary = df.salary.map({\"low\":0,\"medium\":1,\"high\":2})"
      ],
      "metadata": {
        "id": "WKdpoDMEpH_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['left', \"Departments \"], axis=1)\n",
        "y = df['left']"
      ],
      "metadata": {
        "id": "8DAjEV5KnjXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate"
      ],
      "metadata": {
        "id": "s2y8h6Ocn5iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam, Adadelta, RMSprop, Nadam\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.losses import binary_crossentropy\n"
      ],
      "metadata": {
        "id": "nuAQra4YoOM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "9qeuJ9meoUJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=101)"
      ],
      "metadata": {
        "id": "9UiQ-SqsoooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "ZNOuzFdyoXDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 101\n",
        "def build_classifier(optimizer, learn_rate):\n",
        "    tf.random.set_seed(seed)\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units=26, activation='relu'))\n",
        "    classifier.add(Dense(units=13, activation='relu'))\n",
        "    classifier.add(Dense(units=7, activation='relu'))\n",
        "    classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "    classifier.compile(optimizer=optimizer(learn_rate),\n",
        "                       loss='binary_crossentropy')\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "U9i6ABMbpCCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
        "                           verbose=1, patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "KRTNdpmSppNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = KerasClassifier(\n",
        "    build_fn=build_classifier, validation_split=0.1, epochs=200)\n",
        "\n",
        "parameters = {'batch_size': [32, 64],\n",
        "              'optimizer': [Adam, RMSprop],\n",
        "              'learn_rate': [0.001, 0.003, 0.005]}\n",
        "\n",
        "grid_model = GridSearchCV(estimator=classifier_model,\n",
        "                          param_grid=parameters,\n",
        "                          scoring='f1',\n",
        "                          cv=2,\n",
        "                          \n",
        "                          verbose=1)\n",
        "\n",
        "grid_model.fit(X_train, y_train, callbacks=[early_stop])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1hzsSiZpsJI",
        "outputId": "cf7914c6-74e0-4cd5-f270-76a727d75a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 3s 4ms/step - loss: 0.5406 - val_loss: 0.4628\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.4282 - val_loss: 0.4049\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.3672 - val_loss: 0.3300\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2887 - val_loss: 0.2382\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2189 - val_loss: 0.1884\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1846 - val_loss: 0.1684\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1727 - val_loss: 0.1534\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1627 - val_loss: 0.1421\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1568 - val_loss: 0.1373\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1533 - val_loss: 0.1338\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1498 - val_loss: 0.1290\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1479 - val_loss: 0.1264\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1445 - val_loss: 0.1255\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1420 - val_loss: 0.1227\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1387 - val_loss: 0.1251\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1374 - val_loss: 0.1192\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1370 - val_loss: 0.1173\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1340 - val_loss: 0.1167\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1332 - val_loss: 0.1172\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1313 - val_loss: 0.1141\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1305 - val_loss: 0.1162\n",
            "Epoch 22/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1299 - val_loss: 0.1154\n",
            "Epoch 23/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1283 - val_loss: 0.1157\n",
            "Epoch 24/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1107\n",
            "Epoch 25/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1265 - val_loss: 0.1115\n",
            "Epoch 26/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1240 - val_loss: 0.1136\n",
            "Epoch 27/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1235 - val_loss: 0.1133\n",
            "Epoch 28/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1222 - val_loss: 0.1134\n",
            "Epoch 29/200\n",
            "155/169 [==========================>...] - ETA: 0s - loss: 0.1209Restoring model weights from the end of the best epoch: 24.\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1233 - val_loss: 0.1163\n",
            "Epoch 29: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.5406 - val_loss: 0.4620\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.4270 - val_loss: 0.4177\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.3635 - val_loss: 0.3537\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2819 - val_loss: 0.2672\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2088 - val_loss: 0.2085\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1728 - val_loss: 0.1986\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1609 - val_loss: 0.1796\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1532 - val_loss: 0.1715\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1459 - val_loss: 0.1703\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1420 - val_loss: 0.1612\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1387 - val_loss: 0.1678\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1349 - val_loss: 0.1688\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1318 - val_loss: 0.1702\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1285 - val_loss: 0.1530\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1654\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1253 - val_loss: 0.1520\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1556\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1211 - val_loss: 0.1622\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 0.1661\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1177 - val_loss: 0.1607\n",
            "Epoch 21/200\n",
            "153/169 [==========================>...] - ETA: 0s - loss: 0.1128Restoring model weights from the end of the best epoch: 16.\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1167 - val_loss: 0.1596\n",
            "Epoch 21: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.5216 - val_loss: 0.4561\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4261 - val_loss: 0.4118\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.3810 - val_loss: 0.3548\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.3280 - val_loss: 0.2975\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.2704 - val_loss: 0.2365\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.2240 - val_loss: 0.2045\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1938 - val_loss: 0.1734\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1777 - val_loss: 0.1686\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1664 - val_loss: 0.1484\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1613 - val_loss: 0.1450\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1556 - val_loss: 0.1400\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1526 - val_loss: 0.1373\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1486 - val_loss: 0.1317\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1458 - val_loss: 0.1309\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1443 - val_loss: 0.1325\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1410 - val_loss: 0.1267\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1404 - val_loss: 0.1236\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1400 - val_loss: 0.1243\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1376 - val_loss: 0.1271\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1366 - val_loss: 0.1184\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1345 - val_loss: 0.1228\n",
            "Epoch 22/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1342 - val_loss: 0.1213\n",
            "Epoch 23/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1333 - val_loss: 0.1205\n",
            "Epoch 24/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1311 - val_loss: 0.1151\n",
            "Epoch 25/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1305 - val_loss: 0.1160\n",
            "Epoch 26/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1303 - val_loss: 0.1156\n",
            "Epoch 27/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1280 - val_loss: 0.1172\n",
            "Epoch 28/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1283 - val_loss: 0.1164\n",
            "Epoch 29/200\n",
            "163/169 [===========================>..] - ETA: 0s - loss: 0.1259Restoring model weights from the end of the best epoch: 24.\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1274 - val_loss: 0.1195\n",
            "Epoch 29: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.5201 - val_loss: 0.4553\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4219 - val_loss: 0.4201\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.3705 - val_loss: 0.3699\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.3103 - val_loss: 0.3045\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2496 - val_loss: 0.2484\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2067 - val_loss: 0.2178\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1819 - val_loss: 0.2044\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1686 - val_loss: 0.1833\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1587 - val_loss: 0.1863\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1507 - val_loss: 0.1740\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1457 - val_loss: 0.1684\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1420 - val_loss: 0.1885\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1390 - val_loss: 0.1654\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1349 - val_loss: 0.1622\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1333 - val_loss: 0.1651\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 5ms/step - loss: 0.1314 - val_loss: 0.1595\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1287 - val_loss: 0.1599\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1699\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1260 - val_loss: 0.1766\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.1653\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1235 - val_loss: 0.1541\n",
            "Epoch 22/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1224 - val_loss: 0.1656\n",
            "Epoch 23/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1225 - val_loss: 0.1622\n",
            "Epoch 24/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1210 - val_loss: 0.1566\n",
            "Epoch 25/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1194 - val_loss: 0.1727\n",
            "Epoch 26/200\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.1203Restoring model weights from the end of the best epoch: 21.\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1200 - val_loss: 0.1649\n",
            "Epoch 26: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4615 - val_loss: 0.3636\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2687 - val_loss: 0.1919\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 0.1432\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1578 - val_loss: 0.1278\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1529 - val_loss: 0.1299\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 0.1425\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1486 - val_loss: 0.1315\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1408 - val_loss: 0.1224\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1390 - val_loss: 0.1135\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1375 - val_loss: 0.1253\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1363 - val_loss: 0.1140\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1328 - val_loss: 0.1096\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1295 - val_loss: 0.1106\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1277 - val_loss: 0.1136\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 0.1157\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1198 - val_loss: 0.1101\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 0.1086\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1174 - val_loss: 0.1076\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1176 - val_loss: 0.1072\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1126 - val_loss: 0.1011\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1146 - val_loss: 0.1048\n",
            "Epoch 22/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1110 - val_loss: 0.1007\n",
            "Epoch 23/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1104 - val_loss: 0.1084\n",
            "Epoch 24/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1081 - val_loss: 0.0998\n",
            "Epoch 25/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1071 - val_loss: 0.1028\n",
            "Epoch 26/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1040 - val_loss: 0.0943\n",
            "Epoch 27/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1034 - val_loss: 0.0991\n",
            "Epoch 28/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1048 - val_loss: 0.1046\n",
            "Epoch 29/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1039 - val_loss: 0.1048\n",
            "Epoch 30/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.0992 - val_loss: 0.1108\n",
            "Epoch 31/200\n",
            "163/169 [===========================>..] - ETA: 0s - loss: 0.1028Restoring model weights from the end of the best epoch: 26.\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1038 - val_loss: 0.1054\n",
            "Epoch 31: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4650 - val_loss: 0.4165\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2741 - val_loss: 0.2150\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1665 - val_loss: 0.1852\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1477 - val_loss: 0.1619\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1407 - val_loss: 0.1724\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1349 - val_loss: 0.1758\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1332 - val_loss: 0.1668\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1557\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1220 - val_loss: 0.1678\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1173 - val_loss: 0.1487\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1173 - val_loss: 0.1643\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1157 - val_loss: 0.1849\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1160 - val_loss: 0.1735\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1108 - val_loss: 0.1468\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1100 - val_loss: 0.1537\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1083 - val_loss: 0.1438\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1087 - val_loss: 0.1461\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1052 - val_loss: 0.1616\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1030 - val_loss: 0.1671\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1022 - val_loss: 0.1601\n",
            "Epoch 21/200\n",
            "165/169 [============================>.] - ETA: 0s - loss: 0.0993Restoring model weights from the end of the best epoch: 16.\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1008 - val_loss: 0.1545\n",
            "Epoch 21: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4389 - val_loss: 0.3568\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2761 - val_loss: 0.1922\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1790 - val_loss: 0.1495\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1617 - val_loss: 0.1353\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1546 - val_loss: 0.1407\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1491 - val_loss: 0.1213\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1472 - val_loss: 0.1234\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1427 - val_loss: 0.1485\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1372 - val_loss: 0.1138\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1383 - val_loss: 0.1188\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1349 - val_loss: 0.1179\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1343 - val_loss: 0.1150\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1296 - val_loss: 0.1137\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1295 - val_loss: 0.1090\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1271 - val_loss: 0.1210\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1247 - val_loss: 0.1590\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1231 - val_loss: 0.1066\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.1560\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1219 - val_loss: 0.1087\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1235 - val_loss: 0.1034\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1196 - val_loss: 0.1160\n",
            "Epoch 22/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1214 - val_loss: 0.1075\n",
            "Epoch 23/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1209 - val_loss: 0.1113\n",
            "Epoch 24/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1181 - val_loss: 0.1043\n",
            "Epoch 25/200\n",
            "159/169 [===========================>..] - ETA: 0s - loss: 0.1161Restoring model weights from the end of the best epoch: 20.\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1172 - val_loss: 0.1059\n",
            "Epoch 25: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4397 - val_loss: 0.3696\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.2713 - val_loss: 0.2278\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1831 - val_loss: 0.1951\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1599 - val_loss: 0.1804\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1499 - val_loss: 0.1700\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1421 - val_loss: 0.1973\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1367 - val_loss: 0.1678\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1352 - val_loss: 0.1613\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1313 - val_loss: 0.2199\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1299 - val_loss: 0.1753\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1277 - val_loss: 0.1686\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1269 - val_loss: 0.1721\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1235 - val_loss: 0.1502\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1228 - val_loss: 0.1561\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1215 - val_loss: 0.1560\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1212 - val_loss: 0.1538\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1190 - val_loss: 0.1563\n",
            "Epoch 18/200\n",
            "162/169 [===========================>..] - ETA: 0s - loss: 0.1193Restoring model weights from the end of the best epoch: 13.\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1185 - val_loss: 0.1535\n",
            "Epoch 18: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4417 - val_loss: 0.3352\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2532 - val_loss: 0.1740\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1786 - val_loss: 0.1336\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1544 - val_loss: 0.1250\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1482 - val_loss: 0.1282\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1445 - val_loss: 0.1694\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1440 - val_loss: 0.1244\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1362 - val_loss: 0.1302\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1329 - val_loss: 0.1170\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1320 - val_loss: 0.1305\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1324 - val_loss: 0.1090\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1267 - val_loss: 0.1062\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1244 - val_loss: 0.1208\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1213 - val_loss: 0.1433\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1246 - val_loss: 0.1043\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1177 - val_loss: 0.1075\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1150 - val_loss: 0.0977\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1101 - val_loss: 0.1042\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1113 - val_loss: 0.1073\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1086 - val_loss: 0.0960\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1090 - val_loss: 0.1121\n",
            "Epoch 22/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1065 - val_loss: 0.0956\n",
            "Epoch 23/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1073 - val_loss: 0.1006\n",
            "Epoch 24/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1023 - val_loss: 0.0946\n",
            "Epoch 25/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1034 - val_loss: 0.0873\n",
            "Epoch 26/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1017 - val_loss: 0.1032\n",
            "Epoch 27/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.0981 - val_loss: 0.0919\n",
            "Epoch 28/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1026 - val_loss: 0.1187\n",
            "Epoch 29/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1007 - val_loss: 0.0966\n",
            "Epoch 30/200\n",
            "162/169 [===========================>..] - ETA: 0s - loss: 0.0987Restoring model weights from the end of the best epoch: 25.\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.0972 - val_loss: 0.0950\n",
            "Epoch 30: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4142 - val_loss: 0.2796\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1932 - val_loss: 0.1783\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1522 - val_loss: 0.1829\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1418 - val_loss: 0.1552\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1371 - val_loss: 0.1612\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1320 - val_loss: 0.1836\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1337 - val_loss: 0.1758\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1632\n",
            "Epoch 9/200\n",
            "167/169 [============================>.] - ETA: 0s - loss: 0.1223Restoring model weights from the end of the best epoch: 4.\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1225 - val_loss: 0.1593\n",
            "Epoch 9: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.3940 - val_loss: 0.2646\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2089 - val_loss: 0.1542\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1640 - val_loss: 0.1324\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1554 - val_loss: 0.1316\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1514 - val_loss: 0.1390\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1465 - val_loss: 0.1150\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1452 - val_loss: 0.1177\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1423 - val_loss: 0.1466\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1374 - val_loss: 0.1086\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1360 - val_loss: 0.1221\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1369 - val_loss: 0.1126\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1351 - val_loss: 0.1086\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1292 - val_loss: 0.1062\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1289 - val_loss: 0.1059\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1256 - val_loss: 0.1091\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1245 - val_loss: 0.1700\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1231 - val_loss: 0.1027\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1248 - val_loss: 0.1215\n",
            "Epoch 19/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1224 - val_loss: 0.1134\n",
            "Epoch 20/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1187 - val_loss: 0.1029\n",
            "Epoch 21/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1180 - val_loss: 0.1035\n",
            "Epoch 22/200\n",
            "163/169 [===========================>..] - ETA: 0s - loss: 0.1212Restoring model weights from the end of the best epoch: 17.\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1198 - val_loss: 0.1095\n",
            "Epoch 22: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.4282 - val_loss: 0.3609\n",
            "Epoch 2/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.2479 - val_loss: 0.2268\n",
            "Epoch 3/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1706 - val_loss: 0.2020\n",
            "Epoch 4/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1539 - val_loss: 0.1807\n",
            "Epoch 5/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1461 - val_loss: 0.1690\n",
            "Epoch 6/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1391 - val_loss: 0.1798\n",
            "Epoch 7/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1320 - val_loss: 0.1743\n",
            "Epoch 8/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1310 - val_loss: 0.1773\n",
            "Epoch 9/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1280 - val_loss: 0.1769\n",
            "Epoch 10/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1245 - val_loss: 0.1554\n",
            "Epoch 11/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1224 - val_loss: 0.1529\n",
            "Epoch 12/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1201 - val_loss: 0.1851\n",
            "Epoch 13/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1205 - val_loss: 0.1504\n",
            "Epoch 14/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1155 - val_loss: 0.1449\n",
            "Epoch 15/200\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1171 - val_loss: 0.1471\n",
            "Epoch 16/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1131 - val_loss: 0.1477\n",
            "Epoch 17/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1138 - val_loss: 0.1471\n",
            "Epoch 18/200\n",
            "169/169 [==============================] - 1s 3ms/step - loss: 0.1134 - val_loss: 0.1524\n",
            "Epoch 19/200\n",
            "163/169 [===========================>..] - ETA: 0s - loss: 0.1141Restoring model weights from the end of the best epoch: 14.\n",
            "169/169 [==============================] - 1s 4ms/step - loss: 0.1117 - val_loss: 0.1644\n",
            "Epoch 19: early stopping\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.6042 - val_loss: 0.5097\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4704 - val_loss: 0.4544\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4328 - val_loss: 0.4228\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4005 - val_loss: 0.3820\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3592 - val_loss: 0.3353\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3078 - val_loss: 0.2796\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2605 - val_loss: 0.2256\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2167 - val_loss: 0.1928\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1916 - val_loss: 0.1666\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1770 - val_loss: 0.1556\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 0.1477\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1652 - val_loss: 0.1439\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1584 - val_loss: 0.1381\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1548 - val_loss: 0.1356\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1516 - val_loss: 0.1318\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1501 - val_loss: 0.1289\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1481 - val_loss: 0.1275\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1449 - val_loss: 0.1259\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1435 - val_loss: 0.1270\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1416 - val_loss: 0.1215\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1389 - val_loss: 0.1201\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1376 - val_loss: 0.1200\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1358 - val_loss: 0.1189\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1356 - val_loss: 0.1197\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1345 - val_loss: 0.1183\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 0.1173\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1318 - val_loss: 0.1139\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1300 - val_loss: 0.1179\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1299 - val_loss: 0.1135\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 0.1107\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 0.1161\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 0.1134\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.1108\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 0.1134\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1097\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1242 - val_loss: 0.1081\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1075\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 0.1091\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1183 - val_loss: 0.1065\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1184 - val_loss: 0.1082\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1176 - val_loss: 0.1085\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1188 - val_loss: 0.1049\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1163 - val_loss: 0.1075\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 0.1032\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1153 - val_loss: 0.1052\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1152 - val_loss: 0.1059\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 0.1063\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1156 - val_loss: 0.1033\n",
            "Epoch 49/200\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.1114Restoring model weights from the end of the best epoch: 44.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1114 - val_loss: 0.1044\n",
            "Epoch 49: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.6018 - val_loss: 0.5012\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4707 - val_loss: 0.4561\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4304 - val_loss: 0.4319\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3942 - val_loss: 0.3991\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3489 - val_loss: 0.3571\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2972 - val_loss: 0.3012\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2477 - val_loss: 0.2502\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2041 - val_loss: 0.2149\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1775 - val_loss: 0.1960\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1644 - val_loss: 0.1845\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1575 - val_loss: 0.1843\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1503 - val_loss: 0.1789\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1474 - val_loss: 0.1842\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1446 - val_loss: 0.1649\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1409 - val_loss: 0.1680\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1398 - val_loss: 0.1630\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1371 - val_loss: 0.1600\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1344 - val_loss: 0.1681\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1324 - val_loss: 0.1664\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1314 - val_loss: 0.1602\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1302 - val_loss: 0.1620\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1276 - val_loss: 0.1561\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 0.1565\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 0.1519\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1235 - val_loss: 0.1610\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 0.1651\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 0.1543\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1193 - val_loss: 0.1537\n",
            "Epoch 29/200\n",
            "72/85 [========================>.....] - ETA: 0s - loss: 0.1197Restoring model weights from the end of the best epoch: 24.\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1180 - val_loss: 0.1524\n",
            "Epoch 29: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.5714 - val_loss: 0.4962\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4604 - val_loss: 0.4472\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4245 - val_loss: 0.4159\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3967 - val_loss: 0.3850\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3622 - val_loss: 0.3443\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3245 - val_loss: 0.3115\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2864 - val_loss: 0.2587\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2486 - val_loss: 0.2337\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2176 - val_loss: 0.1950\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1968 - val_loss: 0.1848\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1812 - val_loss: 0.1705\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1724 - val_loss: 0.1620\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1658 - val_loss: 0.1535\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1608 - val_loss: 0.1473\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1578 - val_loss: 0.1466\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1530 - val_loss: 0.1398\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1511 - val_loss: 0.1352\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1483 - val_loss: 0.1422\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1455 - val_loss: 0.1401\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1441 - val_loss: 0.1264\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1412 - val_loss: 0.1281\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1403 - val_loss: 0.1272\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1386 - val_loss: 0.1235\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1368 - val_loss: 0.1219\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 0.1202\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1354 - val_loss: 0.1207\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.1193\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1328 - val_loss: 0.1198\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1315 - val_loss: 0.1204\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1313 - val_loss: 0.1219\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1313 - val_loss: 0.1198\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1304 - val_loss: 0.1172\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1282 - val_loss: 0.1194\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1283 - val_loss: 0.1157\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1279 - val_loss: 0.1122\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.1246\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1267 - val_loss: 0.1164\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.1122\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1202\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1244 - val_loss: 0.1228\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1246 - val_loss: 0.1140\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1128\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1115\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 0.1203\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1221 - val_loss: 0.1131\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1217 - val_loss: 0.1116\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1202 - val_loss: 0.1123\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1198 - val_loss: 0.1092\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1208 - val_loss: 0.1126\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1202 - val_loss: 0.1143\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1196 - val_loss: 0.1132\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1180 - val_loss: 0.1126\n",
            "Epoch 53/200\n",
            "76/85 [=========================>....] - ETA: 0s - loss: 0.1178Restoring model weights from the end of the best epoch: 48.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1192 - val_loss: 0.1334\n",
            "Epoch 53: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.5685 - val_loss: 0.4882\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4590 - val_loss: 0.4491\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4209 - val_loss: 0.4238\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3878 - val_loss: 0.3970\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3527 - val_loss: 0.3612\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3122 - val_loss: 0.3196\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2719 - val_loss: 0.2921\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2378 - val_loss: 0.2481\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2098 - val_loss: 0.2209\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1885 - val_loss: 0.2017\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1749 - val_loss: 0.1897\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1656 - val_loss: 0.2115\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1601 - val_loss: 0.1807\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1530 - val_loss: 0.1764\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1488 - val_loss: 0.1760\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1458 - val_loss: 0.1715\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1421 - val_loss: 0.1672\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1402 - val_loss: 0.1774\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1368 - val_loss: 0.1754\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1356 - val_loss: 0.1710\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1334 - val_loss: 0.1615\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1319 - val_loss: 0.1648\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1310 - val_loss: 0.1729\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1294 - val_loss: 0.1594\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1283 - val_loss: 0.1682\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1268 - val_loss: 0.1696\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1622\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1562\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1236 - val_loss: 0.1557\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.1674\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1216 - val_loss: 0.1558\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1213 - val_loss: 0.1550\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1211 - val_loss: 0.1522\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1197 - val_loss: 0.1678\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1188 - val_loss: 0.1600\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1186 - val_loss: 0.1580\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1169 - val_loss: 0.1652\n",
            "Epoch 38/200\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 0.1139Restoring model weights from the end of the best epoch: 33.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1175 - val_loss: 0.1808\n",
            "Epoch 38: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.5143 - val_loss: 0.4372\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3899 - val_loss: 0.3305\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.2658 - val_loss: 0.1953\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.1891 - val_loss: 0.1568\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.1656 - val_loss: 0.1379\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.1592 - val_loss: 0.1306\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1546 - val_loss: 0.1229\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1479 - val_loss: 0.1273\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1473 - val_loss: 0.1179\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1425 - val_loss: 0.1186\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1409 - val_loss: 0.1139\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1391 - val_loss: 0.1182\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1370 - val_loss: 0.1149\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1351 - val_loss: 0.1106\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1326 - val_loss: 0.1164\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1313 - val_loss: 0.1104\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1320 - val_loss: 0.1080\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1270 - val_loss: 0.1103\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1277 - val_loss: 0.1082\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1050\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1274 - val_loss: 0.1131\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.1095\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1237 - val_loss: 0.1131\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1232 - val_loss: 0.1031\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1241 - val_loss: 0.1074\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1199 - val_loss: 0.1071\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 0.1197\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 0.1223\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - ETA: 0s - loss: 0.1191Restoring model weights from the end of the best epoch: 24.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1191 - val_loss: 0.1200\n",
            "Epoch 29: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.5106 - val_loss: 0.4408\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3755 - val_loss: 0.3475\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2522 - val_loss: 0.2357\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 0.1981\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1518 - val_loss: 0.1703\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1439 - val_loss: 0.1909\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1458 - val_loss: 0.1654\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1342 - val_loss: 0.1593\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1319 - val_loss: 0.1680\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 0.1587\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 0.1638\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.1693\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1754\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1196 - val_loss: 0.1569\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 0.1640\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1180 - val_loss: 0.1514\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 0.1584\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1149 - val_loss: 0.1578\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1137 - val_loss: 0.1632\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1127 - val_loss: 0.1534\n",
            "Epoch 21/200\n",
            "71/85 [========================>.....] - ETA: 0s - loss: 0.1112Restoring model weights from the end of the best epoch: 16.\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 0.1658\n",
            "Epoch 21: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.4769 - val_loss: 0.4202\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3657 - val_loss: 0.3085\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2596 - val_loss: 0.2042\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1932 - val_loss: 0.1671\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1723 - val_loss: 0.1742\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1632 - val_loss: 0.1440\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1555 - val_loss: 0.1433\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1511 - val_loss: 0.1571\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1448 - val_loss: 0.1221\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1444 - val_loss: 0.1315\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1397 - val_loss: 0.1262\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1370 - val_loss: 0.1236\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1340 - val_loss: 0.1160\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1327 - val_loss: 0.1160\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1323 - val_loss: 0.1284\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1295 - val_loss: 0.1228\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1280 - val_loss: 0.1201\n",
            "Epoch 18/200\n",
            "75/85 [=========================>....] - ETA: 0s - loss: 0.1264Restoring model weights from the end of the best epoch: 13.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1298 - val_loss: 0.1813\n",
            "Epoch 18: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.4761 - val_loss: 0.4245\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3579 - val_loss: 0.3338\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2506 - val_loss: 0.2424\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1840 - val_loss: 0.1950\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1608 - val_loss: 0.1839\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1498 - val_loss: 0.2221\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1443 - val_loss: 0.1720\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1380 - val_loss: 0.1679\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1358 - val_loss: 0.2027\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1307 - val_loss: 0.1641\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1299 - val_loss: 0.1612\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1275 - val_loss: 0.1913\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1284 - val_loss: 0.1563\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1227 - val_loss: 0.1636\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1220 - val_loss: 0.1593\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1212 - val_loss: 0.1547\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1184 - val_loss: 0.1520\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1185 - val_loss: 0.1646\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1161 - val_loss: 0.1648\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 0.1720\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1146 - val_loss: 0.1470\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1142 - val_loss: 0.1544\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1154 - val_loss: 0.1504\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1128 - val_loss: 0.1495\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1126 - val_loss: 0.1830\n",
            "Epoch 26/200\n",
            "80/85 [===========================>..] - ETA: 0s - loss: 0.1097Restoring model weights from the end of the best epoch: 21.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1118 - val_loss: 0.1523\n",
            "Epoch 26: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.4864 - val_loss: 0.4216\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3448 - val_loss: 0.2531\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2061 - val_loss: 0.1560\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1690 - val_loss: 0.1400\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 0.1303\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 0.1281\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1507 - val_loss: 0.1252\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1441 - val_loss: 0.1202\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1443 - val_loss: 0.1207\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1390 - val_loss: 0.1151\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1354 - val_loss: 0.1142\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1334 - val_loss: 0.1109\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 0.1139\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1279 - val_loss: 0.1057\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 0.1103\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 0.1065\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.1036\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 0.1169\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 0.1034\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 0.1066\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1189 - val_loss: 0.1113\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 0.1065\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 0.1004\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 0.0967\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1122 - val_loss: 0.1077\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1010\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.1065\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 0.1088\n",
            "Epoch 29/200\n",
            "71/85 [========================>.....] - ETA: 0s - loss: 0.1049Restoring model weights from the end of the best epoch: 24.\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1084 - val_loss: 0.1076\n",
            "Epoch 29: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.4881 - val_loss: 0.4220\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.3211 - val_loss: 0.2606\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1805 - val_loss: 0.2109\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1550 - val_loss: 0.1890\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1470 - val_loss: 0.1743\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1423 - val_loss: 0.1962\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 0.1738\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1356 - val_loss: 0.1620\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 0.1704\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 0.1602\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 0.1689\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 0.1637\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1226 - val_loss: 0.1641\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1210 - val_loss: 0.1593\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1214 - val_loss: 0.1707\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1194 - val_loss: 0.1530\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1220 - val_loss: 0.1604\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 0.1584\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1154 - val_loss: 0.2021\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1161 - val_loss: 0.1580\n",
            "Epoch 21/200\n",
            "84/85 [============================>.] - ETA: 0s - loss: 0.1215Restoring model weights from the end of the best epoch: 16.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1222 - val_loss: 0.1533\n",
            "Epoch 21: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.4472 - val_loss: 0.3916\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2989 - val_loss: 0.2035\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.1877 - val_loss: 0.1565\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1625 - val_loss: 0.1382\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1562 - val_loss: 0.1585\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1508 - val_loss: 0.1169\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1452 - val_loss: 0.1252\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1440 - val_loss: 0.1520\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1374 - val_loss: 0.1179\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1382 - val_loss: 0.1421\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1354 - val_loss: 0.1139\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 0.1165\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1290 - val_loss: 0.1077\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1273 - val_loss: 0.1051\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 0.1128\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1249 - val_loss: 0.1137\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1230 - val_loss: 0.1054\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1500\n",
            "Epoch 19/200\n",
            "76/85 [=========================>....] - ETA: 0s - loss: 0.1188Restoring model weights from the end of the best epoch: 14.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1203 - val_loss: 0.1057\n",
            "Epoch 19: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 5ms/step - loss: 0.4392 - val_loss: 0.3708\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.2626 - val_loss: 0.2398\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1805 - val_loss: 0.2012\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1580 - val_loss: 0.1959\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1464 - val_loss: 0.1662\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1405 - val_loss: 0.2543\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1359 - val_loss: 0.1633\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1311 - val_loss: 0.1972\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1298 - val_loss: 0.2325\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1240 - val_loss: 0.1539\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1247 - val_loss: 0.1744\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1221 - val_loss: 0.2109\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1520\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1182 - val_loss: 0.1783\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1198 - val_loss: 0.1573\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1181 - val_loss: 0.1498\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1156 - val_loss: 0.1514\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1177 - val_loss: 0.1484\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1162 - val_loss: 0.1611\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 0.1966\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1117 - val_loss: 0.1522\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1135 - val_loss: 0.1491\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 0.1481\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1133 - val_loss: 0.1445\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1114 - val_loss: 0.2100\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1112 - val_loss: 0.1608\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1103 - val_loss: 0.1545\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1087 - val_loss: 0.1777\n",
            "Epoch 29/200\n",
            "74/85 [=========================>....] - ETA: 0s - loss: 0.1072Restoring model weights from the end of the best epoch: 24.\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1052 - val_loss: 0.1547\n",
            "Epoch 29: early stopping\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n",
            "338/338 [==============================] - 2s 4ms/step - loss: 0.3746 - val_loss: 0.2017\n",
            "Epoch 2/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1654 - val_loss: 0.1827\n",
            "Epoch 3/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1479 - val_loss: 0.1629\n",
            "Epoch 4/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1407 - val_loss: 0.1511\n",
            "Epoch 5/200\n",
            "338/338 [==============================] - 1s 4ms/step - loss: 0.1302 - val_loss: 0.1503\n",
            "Epoch 6/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1254 - val_loss: 0.1477\n",
            "Epoch 7/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1201 - val_loss: 0.1371\n",
            "Epoch 8/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1168 - val_loss: 0.1409\n",
            "Epoch 9/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1146 - val_loss: 0.1431\n",
            "Epoch 10/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1119 - val_loss: 0.1285\n",
            "Epoch 11/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1116 - val_loss: 0.1359\n",
            "Epoch 12/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1088 - val_loss: 0.1291\n",
            "Epoch 13/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1083 - val_loss: 0.1348\n",
            "Epoch 14/200\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1068 - val_loss: 0.1459\n",
            "Epoch 15/200\n",
            "330/338 [============================>.] - ETA: 0s - loss: 0.1050Restoring model weights from the end of the best epoch: 10.\n",
            "338/338 [==============================] - 1s 3ms/step - loss: 0.1053 - val_loss: 0.1343\n",
            "Epoch 15: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=2,\n",
              "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fef32f58050>,\n",
              "             param_grid={'batch_size': [32, 64],\n",
              "                         'learn_rate': [0.001, 0.003, 0.005],\n",
              "                         'optimizer': [<class 'keras.optimizers.optimizer_v2.adam.Adam'>,\n",
              "                                       <class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>]},\n",
              "             scoring='f1', verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_model.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unRkgFIAuP7Y",
        "outputId": "baea1da1-fd9f-4384-e31c-392977dd0e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 32,\n",
              " 'learn_rate': 0.003,\n",
              " 'optimizer': keras.optimizers.optimizer_v2.adam.Adam}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_model.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgtzqXwb1N8I",
        "outputId": "ff327404-4ff9-4bc9-c8ba-3d5c35b781b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.wrappers.scikit_learn.KerasClassifier at 0x7fef32f98250>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = grid_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii_Po-5I1rSF",
        "outputId": "a8865827-182e-4798-bc93-06768645ed2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
        "\n",
        "balanced_accuracy_score(y_test,ypred), accuracy_score(y_test,ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CiGPbvS2Aha",
        "outputId": "4db94530-0778-4edb-dbc4-cbfd0cbfec8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9482711719858548, 0.9666666666666667)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "EmJKhXQE2PSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3fab82-28c9-48e4-9246-c16ecaef1b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     || 348 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     || 209 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     || 81 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.42)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     || 78 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3.post0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     || 112 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     || 50 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     || 147 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=12135524e160c6b17a6651b40ac765d21372a22ee46b0c5d7e932ad0cf1e0ce9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "7sTFdrmLKNUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import *"
      ],
      "metadata": {
        "id": "U_KVo32sKqsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "8-AkrbRCKqiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "EPOCHS = 30 # number of epocs per trial\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "def objective(trial):\n",
        "    \n",
        "    # Clear clutter from previous session graphs.\n",
        "    keras.backend.clear_session()\n",
        "    \n",
        "    # with tf.distribute.strategy.scope():\n",
        "    # Generate our trial model.\n",
        "    classifier = Sequential()\n",
        "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\"])\n",
        "    units_1 = trial.suggest_int(\"units_1\",16,32,1)\n",
        "    classifier.add(Dense(units=units_1, activation=activation))\n",
        "    units_2 = trial.suggest_int(\"units_2\",8,16,1)\n",
        "    classifier.add(Dense(units=units_2, activation=activation))\n",
        "    units_3 = trial.suggest_int(\"units_3\",4,8,1)\n",
        "    classifier.add(Dense(units=7, activation=activation))\n",
        "    classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "    optimizer = trial.suggest_categorical(\"optimizer\", [Adam, RMSprop])\n",
        "    learn_rate = trial.suggest_float(\"learn_rate\",0.0,0.3)\n",
        "    classifier.compile(optimizer=optimizer(learn_rate),\n",
        "                       loss='binary_crossentropy', metrics=[tf.keras.metrics.Recall()])\n",
        "   \n",
        "    # Fit the model on the training data.\n",
        "    # The TFKerasPruningCallback checks for pruning condition every epoch.\n",
        "    BATCH_SIZE = trial.suggest_categorical(\"BATCH_SIZE\", [32, 64])\n",
        "    classifier.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[TFKerasPruningCallback(trial, \"val_loss\")],\n",
        "        epochs=EPOCHS,\n",
        "        validation_split = .15,  #validation_data=(X_test, y_test),\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    # Evaluate the model accuracy on the validation set.\n",
        "    score = classifier.evaluate(X_test, y_test, verbose=1)\n",
        "    return score[1]"
      ],
      "metadata": {
        "id": "NvPQJ7kCJcmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
        "study.optimize(objective, n_trials=10)\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W819yD8LKGBH",
        "outputId": "5788072c-b840-4488-e0c1-283e8abd5919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 07:57:55,105]\u001b[0m A new study created in memory with name: no-name-6282144f-0cb8-484a-943e-b2ddb755acf8\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'keras.optimizers.optimizer_v2.adam.Adam'> which is of type type.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'> which is of type type.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "319/319 [==============================] - 3s 5ms/step - loss: 0.5545 - recall: 0.0058 - val_loss: 0.5614 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "319/319 [==============================] - 1s 5ms/step - loss: 0.5487 - recall: 0.0000e+00 - val_loss: 0.5720 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "319/319 [==============================] - 2s 7ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5657 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "319/319 [==============================] - 3s 8ms/step - loss: 0.5494 - recall: 0.0000e+00 - val_loss: 0.5732 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "319/319 [==============================] - 3s 8ms/step - loss: 0.5492 - recall: 0.0000e+00 - val_loss: 0.5621 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "319/319 [==============================] - 2s 7ms/step - loss: 0.5497 - recall: 0.0000e+00 - val_loss: 0.5628 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "319/319 [==============================] - 2s 7ms/step - loss: 0.5496 - recall: 0.0000e+00 - val_loss: 0.5667 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "319/319 [==============================] - 2s 8ms/step - loss: 0.5492 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "319/319 [==============================] - 2s 7ms/step - loss: 0.5492 - recall: 0.0000e+00 - val_loss: 0.5614 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5493 - recall: 0.0000e+00 - val_loss: 0.5642 - val_recall: 0.0000e+00\n",
            "Epoch 11/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5496 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 12/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5494 - recall: 0.0000e+00 - val_loss: 0.5676 - val_recall: 0.0000e+00\n",
            "Epoch 13/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5494 - recall: 0.0000e+00 - val_loss: 0.5663 - val_recall: 0.0000e+00\n",
            "Epoch 14/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5500 - recall: 0.0000e+00 - val_loss: 0.5669 - val_recall: 0.0000e+00\n",
            "Epoch 15/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5613 - val_recall: 0.0000e+00\n",
            "Epoch 16/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5491 - recall: 0.0000e+00 - val_loss: 0.5657 - val_recall: 0.0000e+00\n",
            "Epoch 17/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 18/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5500 - recall: 0.0000e+00 - val_loss: 0.5652 - val_recall: 0.0000e+00\n",
            "Epoch 19/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5490 - recall: 0.0000e+00 - val_loss: 0.5615 - val_recall: 0.0000e+00\n",
            "Epoch 20/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5489 - recall: 0.0000e+00 - val_loss: 0.5646 - val_recall: 0.0000e+00\n",
            "Epoch 21/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5488 - recall: 0.0000e+00 - val_loss: 0.5614 - val_recall: 0.0000e+00\n",
            "Epoch 22/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5496 - recall: 0.0000e+00 - val_loss: 0.5720 - val_recall: 0.0000e+00\n",
            "Epoch 23/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5618 - val_recall: 0.0000e+00\n",
            "Epoch 24/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5493 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 25/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5493 - recall: 0.0000e+00 - val_loss: 0.5613 - val_recall: 0.0000e+00\n",
            "Epoch 26/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5619 - val_recall: 0.0000e+00\n",
            "Epoch 27/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5706 - val_recall: 0.0000e+00\n",
            "Epoch 28/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5496 - recall: 0.0000e+00 - val_loss: 0.5627 - val_recall: 0.0000e+00\n",
            "Epoch 29/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5495 - recall: 0.0000e+00 - val_loss: 0.5628 - val_recall: 0.0000e+00\n",
            "Epoch 30/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5499 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.5488 - recall: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 07:59:18,616]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'activation': 'relu', 'units_1': 16, 'units_2': 14, 'units_3': 7, 'optimizer': <class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 'learn_rate': 0.13745633830485332, 'BATCH_SIZE': 32}. Best is trial 0 with value: 0.0.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "160/160 [==============================] - 2s 6ms/step - loss: 0.5703 - recall: 0.0502 - val_loss: 0.5695 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.4561 - recall: 0.3117 - val_loss: 0.3908 - val_recall: 0.8795\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.3221 - recall: 0.6928 - val_loss: 0.3482 - val_recall: 0.9040\n",
            "Epoch 4/30\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.2349 - recall: 0.8315 - val_loss: 0.3160 - val_recall: 0.9375\n",
            "Epoch 5/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.2382 - recall: 0.8688 - val_loss: 0.2532 - val_recall: 0.9263\n",
            "Epoch 6/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.1861 - recall: 0.8850 - val_loss: 0.2128 - val_recall: 0.9241\n",
            "Epoch 7/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1850 - recall: 0.8908 - val_loss: 0.1789 - val_recall: 0.8884\n",
            "Epoch 8/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1949 - recall: 0.8917 - val_loss: 0.1636 - val_recall: 0.9040\n",
            "Epoch 9/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1736 - recall: 0.8917 - val_loss: 0.2988 - val_recall: 0.7009\n",
            "Epoch 10/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1885 - recall: 0.8858 - val_loss: 0.1882 - val_recall: 0.8951\n",
            "Epoch 11/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1916 - recall: 0.8817 - val_loss: 0.2161 - val_recall: 0.8371\n",
            "Epoch 12/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1693 - recall: 0.8879 - val_loss: 0.2223 - val_recall: 0.9018\n",
            "Epoch 13/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.2024 - recall: 0.8809 - val_loss: 0.1868 - val_recall: 0.8772\n",
            "Epoch 14/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1916 - recall: 0.8929 - val_loss: 0.2078 - val_recall: 0.8460\n",
            "Epoch 15/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1913 - recall: 0.8796 - val_loss: 0.1995 - val_recall: 0.8772\n",
            "Epoch 16/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1873 - recall: 0.8892 - val_loss: 0.1522 - val_recall: 0.9129\n",
            "Epoch 17/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1880 - recall: 0.8883 - val_loss: 0.2080 - val_recall: 0.9152\n",
            "Epoch 18/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1947 - recall: 0.8825 - val_loss: 0.1909 - val_recall: 0.9107\n",
            "Epoch 19/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1861 - recall: 0.8917 - val_loss: 0.1895 - val_recall: 0.8951\n",
            "Epoch 20/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5034 - recall: 0.2844 - val_loss: 0.5618 - val_recall: 0.0000e+00\n",
            "Epoch 21/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5578 - recall: 0.0000e+00 - val_loss: 0.5552 - val_recall: 0.0000e+00\n",
            "Epoch 22/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5625 - recall: 0.0000e+00 - val_loss: 0.6076 - val_recall: 0.0000e+00\n",
            "Epoch 23/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5583 - recall: 0.0000e+00 - val_loss: 0.5716 - val_recall: 0.0000e+00\n",
            "Epoch 24/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5620 - recall: 0.0000e+00 - val_loss: 0.5736 - val_recall: 0.0000e+00\n",
            "Epoch 25/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5656 - recall: 0.0000e+00 - val_loss: 0.5929 - val_recall: 0.0000e+00\n",
            "Epoch 26/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5598 - recall: 0.0000e+00 - val_loss: 0.5632 - val_recall: 0.0000e+00\n",
            "Epoch 27/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5646 - recall: 0.0000e+00 - val_loss: 0.5970 - val_recall: 0.0000e+00\n",
            "Epoch 28/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5671 - recall: 0.0042 - val_loss: 0.5641 - val_recall: 0.0000e+00\n",
            "Epoch 29/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5692 - recall: 0.0000e+00 - val_loss: 0.5756 - val_recall: 0.0000e+00\n",
            "Epoch 30/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5589 - recall: 0.0000e+00 - val_loss: 0.5613 - val_recall: 0.0000e+00\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.5488 - recall: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:00:00,546]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'activation': 'selu', 'units_1': 23, 'units_2': 14, 'units_3': 5, 'optimizer': <class 'keras.optimizers.optimizer_v2.adam.Adam'>, 'learn_rate': 0.1613165366101983, 'BATCH_SIZE': 64}. Best is trial 0 with value: 0.0.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "319/319 [==============================] - 2s 5ms/step - loss: 0.5639 - recall: 4.1511e-04 - val_loss: 0.5621 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5513 - recall: 0.0000e+00 - val_loss: 0.5687 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5505 - recall: 0.0000e+00 - val_loss: 0.5651 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5504 - recall: 0.0000e+00 - val_loss: 0.5646 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5524 - recall: 0.0000e+00 - val_loss: 0.5668 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5513 - recall: 0.0000e+00 - val_loss: 0.5613 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5514 - recall: 0.0000e+00 - val_loss: 0.5614 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5507 - recall: 0.0000e+00 - val_loss: 0.5611 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5516 - recall: 0.0000e+00 - val_loss: 0.5648 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5506 - recall: 0.0000e+00 - val_loss: 0.5637 - val_recall: 0.0000e+00\n",
            "Epoch 11/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5507 - recall: 0.0000e+00 - val_loss: 0.5625 - val_recall: 0.0000e+00\n",
            "Epoch 12/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5501 - recall: 0.0000e+00 - val_loss: 0.5670 - val_recall: 0.0000e+00\n",
            "Epoch 13/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5499 - recall: 0.0000e+00 - val_loss: 0.5705 - val_recall: 0.0000e+00\n",
            "Epoch 14/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5518 - recall: 0.0000e+00 - val_loss: 0.5646 - val_recall: 0.0000e+00\n",
            "Epoch 15/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5528 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 16/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5500 - recall: 0.0000e+00 - val_loss: 0.5611 - val_recall: 0.0000e+00\n",
            "Epoch 17/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5499 - recall: 0.0000e+00 - val_loss: 0.5711 - val_recall: 0.0000e+00\n",
            "Epoch 18/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5520 - recall: 0.0000e+00 - val_loss: 0.5688 - val_recall: 0.0000e+00\n",
            "Epoch 19/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5487 - recall: 0.0000e+00 - val_loss: 0.5656 - val_recall: 0.0000e+00\n",
            "Epoch 20/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5514 - recall: 0.0000e+00 - val_loss: 0.5690 - val_recall: 0.0000e+00\n",
            "Epoch 21/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5507 - recall: 0.0000e+00 - val_loss: 0.5632 - val_recall: 0.0000e+00\n",
            "Epoch 22/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5523 - recall: 0.0000e+00 - val_loss: 0.5666 - val_recall: 0.0000e+00\n",
            "Epoch 23/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5512 - recall: 0.0000e+00 - val_loss: 0.5713 - val_recall: 0.0000e+00\n",
            "Epoch 24/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5508 - recall: 0.0000e+00 - val_loss: 0.5621 - val_recall: 0.0000e+00\n",
            "Epoch 25/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5509 - recall: 0.0000e+00 - val_loss: 0.5758 - val_recall: 0.0000e+00\n",
            "Epoch 26/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5518 - recall: 0.0000e+00 - val_loss: 0.5621 - val_recall: 0.0000e+00\n",
            "Epoch 27/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.5499 - recall: 0.0000e+00 - val_loss: 0.5790 - val_recall: 0.0000e+00\n",
            "Epoch 28/30\n",
            "314/319 [============================>.] - ETA: 0s - loss: 0.5492 - recall: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:00:36,758]\u001b[0m Trial 2 pruned. Trial was pruned at epoch 27.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.4197 - recall: 0.5035 - val_loss: 0.2549 - val_recall: 0.6808\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.2286 - recall: 0.8161 - val_loss: 0.1790 - val_recall: 0.8571\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1843 - recall: 0.8560 - val_loss: 0.2028 - val_recall: 0.8259\n",
            "Epoch 4/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1927 - recall: 0.8472 - val_loss: 0.2493 - val_recall: 0.9219\n",
            "Epoch 5/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1983 - recall: 0.8531 - val_loss: 0.1650 - val_recall: 0.9062\n",
            "Epoch 6/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1860 - recall: 0.8809 - val_loss: 0.2269 - val_recall: 0.7321\n",
            "Epoch 7/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1757 - recall: 0.8576 - val_loss: 0.1538 - val_recall: 0.9174\n",
            "Epoch 8/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.2046 - recall: 0.8385 - val_loss: 0.1941 - val_recall: 0.8616\n",
            "Epoch 9/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1794 - recall: 0.8738 - val_loss: 0.2444 - val_recall: 0.7723\n",
            "Epoch 10/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1888 - recall: 0.8572 - val_loss: 0.1870 - val_recall: 0.9018\n",
            "Epoch 11/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1723 - recall: 0.8705 - val_loss: 0.1529 - val_recall: 0.9107\n",
            "Epoch 12/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1678 - recall: 0.8684 - val_loss: 0.1719 - val_recall: 0.8683\n",
            "Epoch 13/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1945 - recall: 0.8564 - val_loss: 0.1804 - val_recall: 0.8326\n",
            "Epoch 14/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1718 - recall: 0.8701 - val_loss: 0.2072 - val_recall: 0.8237\n",
            "Epoch 15/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1731 - recall: 0.8626 - val_loss: 0.2342 - val_recall: 0.7098\n",
            "Epoch 16/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1651 - recall: 0.8804 - val_loss: 0.1607 - val_recall: 0.8839\n",
            "Epoch 17/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1924 - recall: 0.8701 - val_loss: 0.2201 - val_recall: 0.9152\n",
            "Epoch 18/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1764 - recall: 0.8792 - val_loss: 0.1779 - val_recall: 0.8906\n",
            "Epoch 19/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1924 - recall: 0.8655 - val_loss: 0.2372 - val_recall: 0.9196\n",
            "Epoch 20/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1729 - recall: 0.8796 - val_loss: 0.1613 - val_recall: 0.8326\n",
            "Epoch 21/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1534 - recall: 0.8888 - val_loss: 0.1616 - val_recall: 0.8683\n",
            "Epoch 22/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.2007 - recall: 0.8572 - val_loss: 0.1748 - val_recall: 0.8862\n",
            "Epoch 23/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1694 - recall: 0.8821 - val_loss: 0.1690 - val_recall: 0.8750\n",
            "Epoch 24/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1673 - recall: 0.8792 - val_loss: 0.1730 - val_recall: 0.8371\n",
            "Epoch 25/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1684 - recall: 0.8896 - val_loss: 0.1769 - val_recall: 0.8996\n",
            "Epoch 26/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1851 - recall: 0.8684 - val_loss: 0.2094 - val_recall: 0.8862\n",
            "Epoch 27/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1643 - recall: 0.8834 - val_loss: 0.1428 - val_recall: 0.8795\n",
            "Epoch 28/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1558 - recall: 0.8900 - val_loss: 0.1473 - val_recall: 0.8705\n",
            "Epoch 29/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1647 - recall: 0.8846 - val_loss: 0.1510 - val_recall: 0.9174\n",
            "Epoch 30/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.1631 - recall: 0.8937 - val_loss: 0.1592 - val_recall: 0.9174\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1895 - recall: 0.9300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:00:58,152]\u001b[0m Trial 3 finished with value: 0.9299719929695129 and parameters: {'activation': 'selu', 'units_1': 21, 'units_2': 15, 'units_3': 8, 'optimizer': <class 'keras.optimizers.optimizer_v2.adam.Adam'>, 'learn_rate': 0.10319618640075823, 'BATCH_SIZE': 64}. Best is trial 3 with value: 0.9299719929695129.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "160/160 [==============================] - 2s 6ms/step - loss: 1.0610 - recall: 0.0095 - val_loss: 0.5652 - val_recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5497 - recall: 0.0000e+00 - val_loss: 0.6018 - val_recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5511 - recall: 0.0000e+00 - val_loss: 0.5763 - val_recall: 0.0000e+00\n",
            "Epoch 4/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5503 - recall: 0.0000e+00 - val_loss: 0.6014 - val_recall: 0.0000e+00\n",
            "Epoch 5/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5508 - recall: 0.0000e+00 - val_loss: 0.5653 - val_recall: 0.0000e+00\n",
            "Epoch 6/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5508 - recall: 0.0000e+00 - val_loss: 0.5683 - val_recall: 0.0000e+00\n",
            "Epoch 7/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5510 - recall: 0.0000e+00 - val_loss: 0.5813 - val_recall: 0.0000e+00\n",
            "Epoch 8/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5503 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 9/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5504 - recall: 0.0000e+00 - val_loss: 0.5613 - val_recall: 0.0000e+00\n",
            "Epoch 10/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5498 - recall: 0.0000e+00 - val_loss: 0.5759 - val_recall: 0.0000e+00\n",
            "Epoch 11/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5505 - recall: 0.0000e+00 - val_loss: 0.5667 - val_recall: 0.0000e+00\n",
            "Epoch 12/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5514 - recall: 0.0000e+00 - val_loss: 0.5620 - val_recall: 0.0000e+00\n",
            "Epoch 13/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5499 - recall: 0.0000e+00 - val_loss: 0.5645 - val_recall: 0.0000e+00\n",
            "Epoch 14/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5514 - recall: 0.0000e+00 - val_loss: 0.5710 - val_recall: 0.0000e+00\n",
            "Epoch 15/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5502 - recall: 0.0000e+00 - val_loss: 0.5691 - val_recall: 0.0000e+00\n",
            "Epoch 16/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5497 - recall: 0.0000e+00 - val_loss: 0.5856 - val_recall: 0.0000e+00\n",
            "Epoch 17/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5502 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 18/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5518 - recall: 0.0000e+00 - val_loss: 0.5699 - val_recall: 0.0000e+00\n",
            "Epoch 19/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5505 - recall: 0.0000e+00 - val_loss: 0.5619 - val_recall: 0.0000e+00\n",
            "Epoch 20/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5507 - recall: 0.0000e+00 - val_loss: 0.5629 - val_recall: 0.0000e+00\n",
            "Epoch 21/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5496 - recall: 0.0000e+00 - val_loss: 0.5614 - val_recall: 0.0000e+00\n",
            "Epoch 22/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5508 - recall: 0.0000e+00 - val_loss: 0.5828 - val_recall: 0.0000e+00\n",
            "Epoch 23/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5515 - recall: 0.0000e+00 - val_loss: 0.5613 - val_recall: 0.0000e+00\n",
            "Epoch 24/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5513 - recall: 0.0000e+00 - val_loss: 0.5611 - val_recall: 0.0000e+00\n",
            "Epoch 25/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5503 - recall: 0.0000e+00 - val_loss: 0.5612 - val_recall: 0.0000e+00\n",
            "Epoch 26/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5515 - recall: 0.0000e+00 - val_loss: 0.5622 - val_recall: 0.0000e+00\n",
            "Epoch 27/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5509 - recall: 0.0000e+00 - val_loss: 0.5663 - val_recall: 0.0000e+00\n",
            "Epoch 28/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5507 - recall: 0.0000e+00 - val_loss: 0.5801 - val_recall: 0.0000e+00\n",
            "Epoch 29/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.5515 - recall: 0.0000e+00 - val_loss: 0.5626 - val_recall: 0.0000e+00\n",
            "Epoch 30/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.5517 - recall: 0.0000e+00 - val_loss: 0.5649 - val_recall: 0.0000e+00\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.5506 - recall: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:01:40,304]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'activation': 'relu', 'units_1': 28, 'units_2': 14, 'units_3': 5, 'optimizer': <class 'keras.optimizers.optimizer_v2.rmsprop.RMSprop'>, 'learn_rate': 0.2505553100868929, 'BATCH_SIZE': 64}. Best is trial 3 with value: 0.9299719929695129.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "319/319 [==============================] - 2s 4ms/step - loss: 0.3930 - recall: 0.5106 - val_loss: 0.3991 - val_recall: 0.9397\n",
            "Epoch 2/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.3048 - recall: 0.8099 - val_loss: 0.2537 - val_recall: 0.8304\n",
            "Epoch 3/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2340 - recall: 0.8572 - val_loss: 0.2092 - val_recall: 0.8996\n",
            "Epoch 4/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2282 - recall: 0.8634 - val_loss: 0.2214 - val_recall: 0.8862\n",
            "Epoch 5/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2364 - recall: 0.8618 - val_loss: 0.1965 - val_recall: 0.8817\n",
            "Epoch 6/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2126 - recall: 0.8634 - val_loss: 0.2046 - val_recall: 0.8862\n",
            "Epoch 7/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1933 - recall: 0.8643 - val_loss: 0.2241 - val_recall: 0.8482\n",
            "Epoch 8/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2181 - recall: 0.8593 - val_loss: 0.2214 - val_recall: 0.9018\n",
            "Epoch 9/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2084 - recall: 0.8659 - val_loss: 0.2410 - val_recall: 0.8705\n",
            "Epoch 10/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2128 - recall: 0.8551 - val_loss: 0.1899 - val_recall: 0.8839\n",
            "Epoch 11/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2123 - recall: 0.8572 - val_loss: 0.1843 - val_recall: 0.9085\n",
            "Epoch 12/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1948 - recall: 0.8834 - val_loss: 0.1768 - val_recall: 0.9062\n",
            "Epoch 13/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1977 - recall: 0.8688 - val_loss: 0.1970 - val_recall: 0.8996\n",
            "Epoch 14/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2131 - recall: 0.8738 - val_loss: 0.2346 - val_recall: 0.9040\n",
            "Epoch 15/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1941 - recall: 0.8883 - val_loss: 0.2589 - val_recall: 0.8996\n",
            "Epoch 16/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1919 - recall: 0.8568 - val_loss: 0.2306 - val_recall: 0.8973\n",
            "Epoch 17/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2049 - recall: 0.8775 - val_loss: 0.2439 - val_recall: 0.9129\n",
            "Epoch 18/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1936 - recall: 0.8751 - val_loss: 0.1824 - val_recall: 0.8973\n",
            "Epoch 19/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1990 - recall: 0.8784 - val_loss: 0.2181 - val_recall: 0.8371\n",
            "Epoch 20/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1970 - recall: 0.8821 - val_loss: 0.1853 - val_recall: 0.8259\n",
            "Epoch 21/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2015 - recall: 0.8568 - val_loss: 0.1925 - val_recall: 0.9174\n",
            "Epoch 22/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2047 - recall: 0.8742 - val_loss: 0.1807 - val_recall: 0.8884\n",
            "Epoch 23/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1923 - recall: 0.8763 - val_loss: 0.1872 - val_recall: 0.8862\n",
            "Epoch 24/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1997 - recall: 0.8597 - val_loss: 0.2062 - val_recall: 0.7946\n",
            "Epoch 25/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1962 - recall: 0.8767 - val_loss: 0.2005 - val_recall: 0.9174\n",
            "Epoch 26/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2011 - recall: 0.8796 - val_loss: 0.2143 - val_recall: 0.7612\n",
            "Epoch 27/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2016 - recall: 0.8742 - val_loss: 0.2243 - val_recall: 0.8594\n",
            "Epoch 28/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1943 - recall: 0.8713 - val_loss: 0.2990 - val_recall: 0.9353\n",
            "Epoch 29/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2127 - recall: 0.8730 - val_loss: 0.1841 - val_recall: 0.8906\n",
            "Epoch 30/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2049 - recall: 0.8614 - val_loss: 0.2344 - val_recall: 0.8884\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2187 - recall: 0.9076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:02:19,034]\u001b[0m Trial 5 finished with value: 0.9075630307197571 and parameters: {'activation': 'relu', 'units_1': 25, 'units_2': 12, 'units_3': 6, 'optimizer': <class 'keras.optimizers.optimizer_v2.adam.Adam'>, 'learn_rate': 0.15013146030383476, 'BATCH_SIZE': 32}. Best is trial 3 with value: 0.9299719929695129.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.4140 - recall: 0.4471 - val_loss: 0.3427 - val_recall: 0.8839\n",
            "Epoch 2/30\n",
            "149/160 [==========================>...] - ETA: 0s - loss: 0.3166 - recall: 0.6937"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:02:21,174]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 1.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 0.4322 - recall: 0.4741 - val_loss: 0.3788 - val_recall: 0.6451\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.2453 - recall: 0.8302 - val_loss: 0.1948 - val_recall: 0.8348\n",
            "Epoch 3/30\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 0.2043 - recall: 0.8663 - val_loss: 0.2722 - val_recall: 0.8415\n",
            "Epoch 4/30\n",
            "151/160 [===========================>..] - ETA: 0s - loss: 0.1893 - recall: 0.8747"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:02:25,635]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "319/319 [==============================] - 2s 4ms/step - loss: 0.3525 - recall: 0.6048 - val_loss: 0.2034 - val_recall: 0.8371\n",
            "Epoch 2/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2196 - recall: 0.8364 - val_loss: 0.3282 - val_recall: 0.4174\n",
            "Epoch 3/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2121 - recall: 0.8543 - val_loss: 0.2034 - val_recall: 0.8683\n",
            "Epoch 4/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1932 - recall: 0.8622 - val_loss: 0.2197 - val_recall: 0.9107\n",
            "Epoch 5/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2097 - recall: 0.8543 - val_loss: 0.1720 - val_recall: 0.8705\n",
            "Epoch 6/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2042 - recall: 0.8713 - val_loss: 0.2354 - val_recall: 0.9219\n",
            "Epoch 7/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1822 - recall: 0.8763 - val_loss: 0.1957 - val_recall: 0.7991\n",
            "Epoch 8/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.2012 - recall: 0.8634 - val_loss: 0.1906 - val_recall: 0.9062\n",
            "Epoch 9/30\n",
            "319/319 [==============================] - 1s 4ms/step - loss: 0.1799 - recall: 0.8759 - val_loss: 0.2706 - val_recall: 0.6897\n",
            "Epoch 10/30\n",
            "306/319 [===========================>..] - ETA: 0s - loss: 0.1884 - recall: 0.8588"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:02:38,920]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 9.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "160/160 [==============================] - 2s 6ms/step - loss: 0.6483 - recall: 0.1540 - val_loss: 0.5006 - val_recall: 0.9420\n",
            "Epoch 2/30\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.3431 - recall: 0.7721"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-28 08:02:49,602]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 1.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_trial.params\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7c2hoZzPYAY",
        "outputId": "c803e6e3-52ea-4aac-ad39-922d5a4bbe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'selu',\n",
              " 'units_1': 21,\n",
              " 'units_2': 15,\n",
              " 'units_3': 8,\n",
              " 'optimizer': keras.optimizers.optimizer_v2.adam.Adam,\n",
              " 'learn_rate': 0.10319618640075823,\n",
              " 'BATCH_SIZE': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "2oPIgJpePpot",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "c8a53f08-fdf7-4519-dfeb-3169edebb274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"eed6f76c-66ae-496f-b8da-b9a200286e87\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"eed6f76c-66ae-496f-b8da-b9a200286e87\")) {                    Plotly.newPlot(                        \"eed6f76c-66ae-496f-b8da-b9a200286e87\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,3,4,5],\"y\":[0.0,0.0,0.9299719929695129,0.0,0.9075630307197571],\"type\":\"scatter\"},{\"name\":\"Best Value\",\"x\":[0,1,3,4,5],\"y\":[0.0,0.0,0.9299719929695129,0.9299719929695129,0.9299719929695129],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('eed6f76c-66ae-496f-b8da-b9a200286e87');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_intermediate_values(study)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ADnApC3CLoub",
        "outputId": "33ac3e18-8f7f-497d-bccc-a1d18ad4e0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"a0df0b42-c023-47a1-9f2f-06e9c07663d6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a0df0b42-c023-47a1-9f2f-06e9c07663d6\")) {                    Plotly.newPlot(                        \"a0df0b42-c023-47a1-9f2f-06e9c07663d6\",                        [{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial0\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.5614423751831055,0.5719883441925049,0.5656985640525818,0.5732000470161438,0.5621022582054138,0.5627999305725098,0.5667125582695007,0.561244785785675,0.5614450573921204,0.564211905002594,0.5612474679946899,0.5675606727600098,0.5663061738014221,0.5668842196464539,0.5612509846687317,0.5656710267066956,0.5611563920974731,0.565181314945221,0.5614890456199646,0.5646078586578369,0.5613566637039185,0.5719640254974365,0.5618387460708618,0.5612139701843262,0.5613198280334473,0.5618568062782288,0.5706234574317932,0.5627138614654541,0.5627996325492859,0.5611957311630249],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial1\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.5695311427116394,0.39075830578804016,0.34821340441703796,0.31597381830215454,0.2532111406326294,0.21276995539665222,0.17886649072170258,0.16358870267868042,0.2988090217113495,0.18821436166763306,0.2160996049642563,0.22226206958293915,0.1868392378091812,0.20775336027145386,0.19945524632930756,0.15221723914146423,0.20800970494747162,0.19085471332073212,0.18951921164989471,0.5618493556976318,0.5551846027374268,0.6076292395591736,0.5716465711593628,0.5736063718795776,0.5929243564605713,0.5632240176200867,0.5970394611358643,0.5640522837638855,0.575600266456604,0.5613247156143188],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial2\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],\"y\":[0.5620758533477783,0.5686871409416199,0.5650923252105713,0.5646356344223022,0.5667533278465271,0.5613456964492798,0.5613680481910706,0.5611132979393005,0.5647866129875183,0.5637083053588867,0.5624870657920837,0.5669935345649719,0.5705267190933228,0.5645768046379089,0.5612409710884094,0.561115026473999,0.5710549354553223,0.5688046216964722,0.5655856132507324,0.5690463185310364,0.5631951093673706,0.566644012928009,0.5712629556655884,0.5620991587638855,0.5758140087127686,0.5620670914649963,0.5789898037910461,0.5636942982673645],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial3\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.2549149990081787,0.17898383736610413,0.20278804004192352,0.2492981106042862,0.16502006351947784,0.2269199788570404,0.1538013219833374,0.19407396018505096,0.24435880780220032,0.1870332807302475,0.15285181999206543,0.17188984155654907,0.18040698766708374,0.20715299248695374,0.23419424891471863,0.16065765917301178,0.22012731432914734,0.17791327834129333,0.2371751070022583,0.16126391291618347,0.16161593794822693,0.1747559756040573,0.16895414888858795,0.17299647629261017,0.17692407965660095,0.209385946393013,0.1428278088569641,0.14731411635875702,0.15101298689842224,0.15917731821537018],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial4\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.5652379393577576,0.6018458604812622,0.5762800574302673,0.6014025211334229,0.5652856826782227,0.5682660341262817,0.5813161134719849,0.5612486004829407,0.5613215565681458,0.5759190320968628,0.5666682124137878,0.5619935393333435,0.5644694566726685,0.5709523558616638,0.5690761208534241,0.5856088995933533,0.5612369775772095,0.5699344277381897,0.5619353652000427,0.5628818273544312,0.5614022016525269,0.5828321576118469,0.5613024830818176,0.5611128807067871,0.5611754059791565,0.562193751335144,0.5662858486175537,0.5800536870956421,0.5625718832015991,0.5648687481880188],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial5\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.3990911543369293,0.25367942452430725,0.20921412110328674,0.22142206132411957,0.19647188484668732,0.20464080572128296,0.22406451404094696,0.22137556970119476,0.24104256927967072,0.18988290429115295,0.18434415757656097,0.17682240903377533,0.19696815311908722,0.23455235362052917,0.2588976323604584,0.23058483004570007,0.24394020438194275,0.18240061402320862,0.21810518205165863,0.18533459305763245,0.1925308108329773,0.18073782324790955,0.18724654614925385,0.20623381435871124,0.20052480697631836,0.21425530314445496,0.22429092228412628,0.298994243144989,0.18413138389587402,0.2343893051147461],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial6\",\"x\":[0,1],\"y\":[0.342744916677475,0.34878018498420715],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial7\",\"x\":[0,1,2,3],\"y\":[0.37877023220062256,0.19480924308300018,0.27215492725372314,0.22261880338191986],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial8\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.20338843762874603,0.328235000371933,0.20340456068515778,0.21972553431987762,0.17199121415615082,0.23539753258228302,0.19569028913974762,0.19059813022613525,0.27055174112319946,0.18281421065330505],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial9\",\"x\":[0,1],\"y\":[0.5005921721458435,0.35137227177619934],\"type\":\"scatter\"}],                        {\"showlegend\":false,\"title\":{\"text\":\"Intermediate Values Plot\"},\"xaxis\":{\"title\":{\"text\":\"Step\"}},\"yaxis\":{\"title\":{\"text\":\"Intermediate Value\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a0df0b42-c023-47a1-9f2f-06e9c07663d6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_param_importances(study)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "EsNudQ82L1PP",
        "outputId": "904438dd-a93f-4e4b-b086-719fe8087739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"f1b52507-a90c-4b94-b78e-2dc3dfb013fa\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f1b52507-a90c-4b94-b78e-2dc3dfb013fa\")) {                    Plotly.newPlot(                        \"f1b52507-a90c-4b94-b78e-2dc3dfb013fa\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"BATCH_SIZE (CategoricalDistribution): 0.016829865172036704<extra></extra>\",\"activation (CategoricalDistribution): 0.01683520920231573<extra></extra>\",\"units_1 (IntDistribution): 0.08984437713461906<extra></extra>\",\"optimizer (CategoricalDistribution): 0.11602162619048757<extra></extra>\",\"units_3 (IntDistribution): 0.18830798355982958<extra></extra>\",\"units_2 (IntDistribution): 0.2340003067459066<extra></extra>\",\"learn_rate (FloatDistribution): 0.3381606319948046<extra></extra>\"],\"marker\":{\"color\":\"rgb(66,146,198)\"},\"orientation\":\"h\",\"text\":[\"0.02\",\"0.02\",\"0.09\",\"0.12\",\"0.19\",\"0.23\",\"0.34\"],\"textposition\":\"outside\",\"x\":[0.016829865172036704,0.01683520920231573,0.08984437713461906,0.11602162619048757,0.18830798355982958,0.2340003067459066,0.3381606319948046],\"y\":[\"BATCH_SIZE\",\"activation\",\"units_1\",\"optimizer\",\"units_3\",\"units_2\",\"learn_rate\"],\"type\":\"bar\"}],                        {\"showlegend\":false,\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Importance for Objective Value\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f1b52507-a90c-4b94-b78e-2dc3dfb013fa');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c25Oe20OMAk5",
        "outputId": "bd794a59-a68f-44d0-e56d-40bb88af0fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  10\n",
            "  Number of pruned trials:  5\n",
            "  Number of complete trials:  5\n",
            "Best trial:\n",
            "  Value:  0.9299719929695129\n",
            "  Params: \n",
            "    activation: selu\n",
            "    units_1: 21\n",
            "    units_2: 15\n",
            "    units_3: 8\n",
            "    optimizer: <class 'keras.optimizers.optimizer_v2.adam.Adam'>\n",
            "    learn_rate: 0.10319618640075823\n",
            "    BATCH_SIZE: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_LV6LG5MK5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}